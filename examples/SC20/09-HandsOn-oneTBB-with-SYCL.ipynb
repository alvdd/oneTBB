{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using oneTBB with SYCL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Sections\n",
    "- [oneTBB Generic Algorithms](#oneTBB-Generic-Algorithms)\n",
    "- [Calculating pi with tbb::parallel_reduce](#Calculating-pi-with-tbb::parallel_reduce)\n",
    "- [Using SYCL on GPU and oneTBB on CPU consecutively](#Using-SYCL-on-GPU-and-oneTBB-on-CPU-consecutively)\n",
    "- [Using tbb::task_group to dispatch GPU and CPU code in parallel](#Using-tbb::task_group-to-dispatch-GPU-and-CPU-code-in-parallel)\n",
    "- [Using resumable tasks or async_node to share the workload across the CPU and GPU](#Using-resumable-tasks-or-async_node-to-share-the-workload-across-the-CPU-and-GPU)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Objectives\n",
    "\n",
    "* Gain experince with oneTBB generic algorithms \n",
    "* Use tbb::parallel_reduce to estimate pi as the area of a unit circle\n",
    "* Learn how to use oneTBB and SYCL together.\n",
    "* Learn how to use a resumable task or async_node to avoid blocking a oneTBB worker thread."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# oneTBB Generic Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While it's possible to implement a parallel application by using oneTBB to specify each individual task that can run\n",
    "concurrently, it is more common to make use of one of its data parallel generic algorithms. The oneTBB library provides \n",
    "a number of [generic parallel algorithms](https://spec.oneapi.com/versions/latest/elements/oneTBB/source/algorithms.html),\n",
    "including `parallel_for`, `parallel_reduce`, `parallel_scan`, `parallel_invoke` and `parallel_pipeline`. These functions \n",
    "capture many of the common parallel patterns that are key to unlocking multithreaded performance on the CPU. \n",
    "\n",
    "In this section, we provide an exercise that will introduce you one example algorithm, `parallel_reduce`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating pi with tbb::parallel_reduce\n",
    "\n",
    "In this exercise, we calculate pi using the approach shown in the figure below. The idea is to\n",
    "compute the area of a unit circle, which is equal to pi. We do this by approximating the area of \n",
    "1/4th of a unit circle, summing up the areas of ``num_intervals`` rectangles that have\n",
    "a height of ``sqrt(1-x*x)`` and a width of ``dx == 1.0/num_intervals``. This sum is multiplied by \n",
    "4 to compute the total area of the unit circle, providing us with an approximation for pi.\n",
    "\n",
    "![Algorithm to compute pi](assets/pi.png)\n",
    "\n",
    "### Run the sequential baseline implementation\n",
    "\n",
    "Before we add any parallelism, let's validate this approach by running a baseline sequential implementation. Inspect \n",
    "the sequential code below - there are no modifications necessary. Run the first cell to create the file, then run the \n",
    "cell below it to compile and execute the code. This represents the baseline sequential result and time for our pi \n",
    "computation exercise.\n",
    "\n",
    "1. Inspect the code cell below, then click run ▶ to save the code to a file\n",
    "2. Run ▶ the cell in the __Build and Run the baseline__ section below the code snippet to compile and execute the code in the saved file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile lab/pi-serial.cpp\n",
    "//==============================================================\n",
    "// Copyright (c) 2020 Intel Corporation\n",
    "//\n",
    "// SPDX-License-Identifier: Apache-2.0\n",
    "// =============================================================\n",
    "\n",
    "#include <chrono>\n",
    "#include <cmath>\n",
    "#include <iostream>\n",
    "#include <limits>\n",
    "\n",
    "double calc_pi(int num_intervals) {\n",
    "  double dx = 1.0 / num_intervals;\n",
    "  double sum = 0.0;\n",
    "  for (int i = 0; i < num_intervals; ++i) {\n",
    "    double x = (i+0.5)*dx;\n",
    "    double h = std::sqrt(1-x*x);\n",
    "    sum += h*dx;\n",
    "  }\n",
    "  double pi = 4 * sum;\n",
    "  return pi;\n",
    "}\n",
    "\n",
    "int main() {\n",
    "  const int num_intervals = std::numeric_limits<int>::max();\n",
    "  double serial_time = 0.0;\n",
    "  {\n",
    "    auto st0 = std::chrono::high_resolution_clock::now();\n",
    "    double pi = calc_pi(num_intervals);\n",
    "    serial_time = 1e-9*(std::chrono::high_resolution_clock::now() - st0).count();\n",
    "    std::cout << \"serial pi == \" << pi << std::endl;\n",
    "  }\n",
    "\n",
    "  std::cout << \"serial_time == \" << serial_time << \" seconds\" << std::endl;\n",
    "  return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build and Run the baseline\n",
    "Select the cell below and click Run ▶ to compile and execute the code above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! chmod 755 q; chmod 755 ./scripts/run_pi-serial.sh; if [ -x \"$(command -v qsub)\" ]; then ./q scripts/run_pi-serial.sh; else ./scripts/run_pi-serial.sh; fi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement a parallel version with tbb::parallel_reduce\n",
    "\n",
    "Our sequential code accumulates values into a single final sum, making it a reduction operation and a match for ``tbb::parallel_reduce``.\n",
    "You can find detailed documentation for ``parallel_reduce`` [here](https://software.intel.com/content/www/us/en/develop/documentation/tbb-documentation/top/intel-threading-building-blocks-developer-reference/algorithms/parallelreduce-template-function.html). Briefly though, a ``parallel_reduce`` runs a user-provided function\n",
    "on chunks of the iteration space, potentially concurrently, resulting in several partial results. In our example, these partial results will be partial sums. These partial results are combined using a user-provided reduction function, in our pi example, `std::plus` might be used (hint). \n",
    "\n",
    "The interface of ``parallel_reduce`` needed for this example is shown below:\n",
    "\n",
    "```cpp\n",
    "template<typename Range, typename Value, typename Func, typename Reduction>\n",
    "Value parallel_reduce( const Range& range, const Value& identity,\n",
    "                       const Func& func, const Reduction& reduction );\n",
    "```\n",
    "\n",
    "The ``range`` object provides the iteration space, which in our example is 0 to num_intervals - 1. ``identity`` is the identity value for the \n",
    "operation that is being parallelized; for a summation, the identity value is 0, since ``sum == sum + 0``. We provide a lambda expression for \n",
    "``func`` to compute the partial results, which in our example will return a partial sum for a given range ``r``, accumulating into the \n",
    "starting value ``init``. Finally, ``reduction`` is the operation to use to combine the partial results.\n",
    "\n",
    "For this exercise, complete the following steps:\n",
    "\n",
    "1. Inspect the code cell below and make the following modifications.\n",
    "  1. Fix the upper bound in the ``tbb::blocked_range``\n",
    "  2. Fix the identity value\n",
    "  3. Add the loop body code\n",
    "  4. Fix the reduction function\n",
    "2. When the modifications are complete, click run ▶ to save the code to a file.\n",
    "3. Run ▶ the cell in the __Build and Run the modified code__ section below the code snippet to compile and execute the code in the saved file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile lab/pi-parallel.cpp\n",
    "//==============================================================\n",
    "// Copyright (c) 2020 Intel Corporation\n",
    "//\n",
    "// SPDX-License-Identifier: Apache-2.0\n",
    "// =============================================================\n",
    "\n",
    "#include <chrono>\n",
    "#include <cmath>\n",
    "#include <iostream>\n",
    "#include <limits>\n",
    "#include <thread>\n",
    "#include <tbb/tbb.h>\n",
    "\n",
    "#define INCORRECT_VALUE 1\n",
    "#define INCORRECT_FUNCTION std::minus<double>()\n",
    "\n",
    "double calc_pi(int num_intervals) {\n",
    "  double dx = 1.0 / num_intervals;\n",
    "  double sum = tbb::parallel_reduce(\n",
    "    /* STEP 1: fix the upper bound: */ tbb::blocked_range<int>(0, INCORRECT_VALUE), \n",
    "    /* STEP 2: provide a proper identity value for summation */ INCORRECT_VALUE,\n",
    "    /* func */ \n",
    "    [=](const tbb::blocked_range<int>& r, double init) -> double {\n",
    "      for (int i = r.begin(); i != r.end(); ++i) {\n",
    "        // STEP 3: Add the loop body code:\n",
    "        //         Hint: it will look a lot like the the sequential code.\n",
    "        //               the returned value should be (init + the_partial_sum)\n",
    "      }\n",
    "      return init;\n",
    "    },\n",
    "    // STEP 4: provide the reduction function\n",
    "    //         Hint, maybe std::plus<double>{}\n",
    "    INCORRECT_FUNCTION\n",
    "  );\n",
    "  double pi = 4 * sum;\n",
    "  return pi;\n",
    "}\n",
    "\n",
    "static void warmupTBB() {\n",
    "  int num_threads = std::thread::hardware_concurrency();\n",
    "  tbb::parallel_for(0, num_threads,\n",
    "    [](unsigned int) { \n",
    "      std::this_thread::sleep_for(std::chrono::milliseconds(10)); \n",
    "  });\n",
    "}\n",
    "\n",
    "int main() {\n",
    "  const int num_intervals = std::numeric_limits<int>::max();\n",
    "  double parallel_time = 0.0;\n",
    "  warmupTBB();\n",
    "  {\n",
    "    auto pt0 = std::chrono::high_resolution_clock::now();\n",
    "    double pi = calc_pi(num_intervals);\n",
    "    parallel_time = 1e-9*(std::chrono::high_resolution_clock::now() - pt0).count();\n",
    "    std::cout << \"parallel pi == \" << pi << std::endl;\n",
    "  }\n",
    "\n",
    "  std::cout << \"parallel_time == \" << parallel_time << \" seconds\" << std::endl;\n",
    "  return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build and Run the modified code\n",
    "\n",
    "Select the cell below and click Run ▶ to compile and execute the code that you modified above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! chmod 755 q; chmod 755 ./scripts/run_pi-parallel.sh; if [ -x \"$(command -v qsub)\" ]; then ./q scripts/run_pi-parallel.sh; else ./scripts/run_pi-parallel.sh; fi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pi Example Solution (Don't peak, unless you have to)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile solutions/pi-parallel.cpp\n",
    "//==============================================================\n",
    "// Copyright (c) 2020 Intel Corporation\n",
    "//\n",
    "// SPDX-License-Identifier: Apache-2.0\n",
    "// =============================================================\n",
    "\n",
    "#include <chrono>\n",
    "#include <cmath>\n",
    "#include <iostream>\n",
    "#include <limits>\n",
    "#include <thread>\n",
    "#include <tbb/tbb.h>\n",
    "\n",
    "double calc_pi(int num_intervals) {\n",
    "  double dx = 1.0 / num_intervals;\n",
    "  double sum = tbb::parallel_reduce(\n",
    "    /* range = */ tbb::blocked_range<int>(0, num_intervals ), \n",
    "    /* identity = */ 0.0,\n",
    "    /* func */ \n",
    "    [=](const tbb::blocked_range<int>& r, double init) -> double {\n",
    "      for (int i = r.begin(); i != r.end(); ++i) {\n",
    "        double x = (i+0.5)*dx;\n",
    "        double h = std::sqrt(1-x*x);\n",
    "        init += h*dx;\n",
    "      }\n",
    "      return init;\n",
    "    },\n",
    "    std::plus<double>{}\n",
    "  );\n",
    "  double pi = 4 * sum;\n",
    "  return pi;\n",
    "}\n",
    "\n",
    "static void warmupTBB() {\n",
    "  int num_threads = std::thread::hardware_concurrency();\n",
    "  tbb::parallel_for(0, num_threads,\n",
    "    [](unsigned int) { \n",
    "      std::this_thread::sleep_for(std::chrono::milliseconds(10)); \n",
    "  });\n",
    "}\n",
    "\n",
    "int main() {\n",
    "  const int num_intervals = std::numeric_limits<int>::max();\n",
    "  double parallel_time = 0.0;\n",
    "  warmupTBB();\n",
    "  {\n",
    "    auto pt0 = std::chrono::high_resolution_clock::now();\n",
    "    double pi = calc_pi(num_intervals);\n",
    "    parallel_time = 1e-9*(std::chrono::high_resolution_clock::now() - pt0).count();\n",
    "    std::cout << \"parallel pi == \" << pi << std::endl;\n",
    "  }\n",
    "\n",
    "  std::cout << \"parallel_time == \" << parallel_time << \" seconds\" << std::endl;\n",
    "  return 0;\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! chmod 755 q; chmod 755 ./scripts/run_pi-solution.sh; if [ -x \"$(command -v qsub)\" ]; then ./q scripts/run_pi-solution.sh; else ./scripts/run_pi-solution.sh; fi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using SYCL on GPU and oneTBB on CPU consecutively"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can look at using oneTBB algorithms in combination with SYCL. \n",
    "\n",
    "Let's start by computing `c = a + alpha * b` (usually known as a `triad` operation), first using a SYCL parallel_for and then a TBB parallel_for. On the **GPU**, we compute `c_sycl = a_array + b_array * alpha`, whereas on the **CPU**, we write to a different result array and compute `c_tbb = a_array + b_array * alpha`. In this example, we are executing these algorithms one after the other, and not overlapping the use of the GPU with the use of the CPU.\n",
    "\n",
    "<img src=\"assets/Triad-GPU-then-CPU.png\" width=\"1000\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Inspect the code cell below, then click run ▶ to save the code to a file\n",
    "2. Run ▶ the cell in the __Build and Run the baseline__ section below the code snippet to compile and execute the code in the saved file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile lab/triad-consecutive.cpp\n",
    "//==============================================================\n",
    "// Copyright (c) 2020 Intel Corporation\n",
    "//\n",
    "// SPDX-License-Identifier: Apache 2.0\n",
    "// =============================================================\n",
    "\n",
    "#include \"../common/common_utils.hpp\"\n",
    "#include <array>\n",
    "#include <tbb/blocked_range.h>\n",
    "#include <tbb/parallel_for.h>\n",
    "\n",
    "int main() {\n",
    "  const float alpha = 0.5;  // alpha for triad calculation\n",
    "  const size_t array_size = 16;\n",
    "\n",
    "  std::array<float, array_size> a_array, b_array, c_sycl, c_tbb;\n",
    "\n",
    "  // sets array values to 0..N\n",
    "  common::init_input_arrays(a_array, b_array); \n",
    "\n",
    "  std::cout << \"executing on the GPU using SYCL\\n\";\n",
    "  {  \n",
    "    sycl::buffer a_buffer{a_array}, b_buffer{b_array}, c_buffer{c_sycl};\n",
    "    sycl::queue q{sycl::default_selector{}};\n",
    "    q.submit([&](sycl::handler& h) {            \n",
    "      sycl::accessor a_accessor{a_buffer, h, sycl::read_only};\n",
    "      sycl::accessor b_accessor{b_buffer, h, sycl::read_only};\n",
    "      sycl::accessor c_accessor{c_buffer, h, sycl::write_only};\n",
    "      h.parallel_for(sycl::range<1>{array_size}, [=](sycl::id<1> index) {\n",
    "         c_accessor[index] = a_accessor[index] + b_accessor[index] * alpha;\n",
    "      });  \n",
    "    }).wait(); //Wait here\n",
    "  }\n",
    "\n",
    "  std::cout << \"executing on the CPU using TBB\\n\";\n",
    "  tbb::parallel_for(tbb::blocked_range<int>(0, a_array.size()),\n",
    "    [&](tbb::blocked_range<int> r) {\n",
    "      for (int index = r.begin(); index < r.end(); ++index) {\n",
    "        c_tbb[index] = a_array[index] + b_array[index] * alpha;\n",
    "      }\n",
    "  });\n",
    "\n",
    "  common::validate_results(alpha, a_array, b_array, c_sycl, c_tbb);\n",
    "  common::print_results(alpha, a_array, b_array, c_sycl, c_tbb);\n",
    "} "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build and Run the baseline\n",
    "Select the cell below and click Run ▶ to compile and execute the code above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! chmod 755 q; chmod 755 ./scripts/run_consecutive.sh; if [ -x \"$(command -v qsub)\" ]; then ./q scripts/run_consecutive.sh; else ./scripts/run_consecutive.sh; fi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using tbb::task_group to dispatch GPU and CPU code in parallel\n",
    "\n",
    "Of course the CPU and the GPU can work in parallel. Our first approach will be to use `tbb::task_group` to spawn a task for the GPU and another concurrent one for the CPU. It will look like:\n",
    "\n",
    "<img src=\"assets/Triad-task_group.png\" width=\"1000\">\n",
    "\n",
    "The class `tbb::task_group` is quite easy to use:\n",
    "\n",
    "```\n",
    "tbb::task_group g; // Create a task_group object g\n",
    "g.run([]{cout << \"One task passed to g.run as a lambda\\n\";});\n",
    "g.run([]{cout << \"Another concurrent task in this lambda\\n\";});\n",
    "g.wait() // Wait for both tasks to complete\n",
    "```\n",
    "\n",
    "For this exercise, complete the following steps:\n",
    "\n",
    "1. Inspect the code cell below and make the following modifications.\n",
    "  1. Complete the body of the lambda for the first call to run, offloading the code to the GPU\n",
    "  2. Complete the body of the lambda for the second call to run, executing the code on the CPU \n",
    "2. When the modifications are complete, click run ▶ to save the code to a file.\n",
    "3. Run ▶ the cell in the __Build and Run the modified code__ section below the code snippet to compile and execute the code in the saved file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile lab/triad-task_group.cpp\n",
    "//==============================================================\n",
    "// Copyright (c) 2020 Intel Corporation\n",
    "//\n",
    "// SPDX-License-Identifier: Apache 2.0\n",
    "// =============================================================\n",
    "\n",
    "#include \"../common/common_utils.hpp\"\n",
    "#include <array>\n",
    "#include <tbb/blocked_range.h>\n",
    "#include <tbb/parallel_for.h>\n",
    "#include \"tbb/task_group.h\"\n",
    "\n",
    "int main() {\n",
    "  const float alpha = 0.5;  // coeff for triad calculation\n",
    "  const size_t array_size = 16;\n",
    "\n",
    "  std::array<float, array_size> a_array, b_array, c_sycl, c_tbb;\n",
    "\n",
    "  // sets array values to 0..N\n",
    "  common::init_input_arrays(a_array, b_array); \n",
    "\n",
    "  // create task_group\n",
    "  tbb::task_group tg;\n",
    "\n",
    "  // Run a TBB task that uses SYCL to offload to GPU, function run does not block\n",
    "  tg.run([&, alpha]() {\n",
    "    std::cout << \"executing on the GPU using SYCL\\n\";\n",
    "    {  \n",
    "      // STEP A: Complete the body to offload to the GPU\n",
    "      //         Hint: look at (copy from) the consecutive calls sample\n",
    "    }\n",
    "  });\n",
    "\n",
    "  // Run a TBB task that uses SYCL to offload to CPU\n",
    "  tg.run([&, alpha]() {\n",
    "    std::cout << \"executing on the CPU using TBB\\n\";\n",
    "    // STEP B: Complete the body to offload to the CPU\n",
    "    //         Hint: look at (copy from) the consecutive calls sample\n",
    "  });\n",
    "\n",
    "  // wait for both TBB tasks to complete\n",
    "  tg.wait();\n",
    "\n",
    "  common::validate_results(alpha, a_array, b_array, c_sycl, c_tbb);\n",
    "  common::print_results(alpha, a_array, b_array, c_sycl, c_tbb);\n",
    "} "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build and Run the modified code\n",
    "\n",
    "Select the cell below and click Run ▶ to compile and execute the code that you modified above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! chmod 755 q; chmod 755 ./scripts/run_tasks.sh; if [ -x \"$(command -v qsub)\" ]; then ./q scripts/run_tasks.sh; else ./scripts/run_tasks.sh; fi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution (Don't peak unless you have to)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%%writefile solutions/triad-task_group-solved.cpp\n",
    "//==============================================================\n",
    "// Copyright (c) 2020 Intel Corporation\n",
    "//\n",
    "// SPDX-License-Identifier: Apache 2.0\n",
    "// =============================================================\n",
    "\n",
    "#include \"../common/common_utils.hpp\"\n",
    "#include <array>\n",
    "#include <tbb/blocked_range.h>\n",
    "#include <tbb/parallel_for.h>\n",
    "#include \"tbb/task_group.h\"\n",
    "\n",
    "int main() {\n",
    "  const float alpha = 0.5;  // coeff for triad calculation\n",
    "  const size_t array_size = 16;\n",
    "\n",
    "  std::array<float, array_size> a_array, b_array, c_sycl, c_tbb;\n",
    "\n",
    "  // sets array values to 0..N\n",
    "  common::init_input_arrays(a_array, b_array); \n",
    "\n",
    "  // create task_group\n",
    "  tbb::task_group tg;\n",
    "\n",
    "  // Run a TBB task that uses SYCL to offload to GPU, function run does not block\n",
    "  tg.run([&, alpha]() {\n",
    "    std::cout << \"executing on the GPU using SYCL\\n\";\n",
    "    {  \n",
    "      sycl::buffer a_buffer{a_array}, b_buffer{b_array}, c_buffer{c_sycl};\n",
    "      sycl::queue q{sycl::default_selector{}};\n",
    "      q.submit([&](sycl::handler& h) {            \n",
    "        sycl::accessor a_accessor{a_buffer, h, sycl::read_only};\n",
    "        sycl::accessor b_accessor{b_buffer, h, sycl::read_only};\n",
    "        sycl::accessor c_accessor{c_buffer, h, sycl::write_only};\n",
    "        h.parallel_for(sycl::range<1>{array_size}, [=](sycl::id<1> index) {\n",
    "           c_accessor[index] = a_accessor[index] + b_accessor[index] * alpha;\n",
    "        });  \n",
    "      }).wait();\n",
    "    }\n",
    "  });\n",
    "\n",
    "  // Run a TBB task that uses SYCL to offload to CPU\n",
    "  tg.run([&, alpha]() {\n",
    "    std::cout << \"executing on the CPU using TBB\\n\";\n",
    "\n",
    "    tbb::parallel_for(tbb::blocked_range<int>(0, a_array.size()),\n",
    "      [&](tbb::blocked_range<int> r) {\n",
    "        for (int index = r.begin(); index < r.end(); ++index) {\n",
    "          c_tbb[index] = a_array[index] + b_array[index] * alpha;\n",
    "        }\n",
    "    });\n",
    "  });\n",
    "\n",
    "  // wait for both TBB tasks to complete\n",
    "  tg.wait();\n",
    "\n",
    "  common::validate_results(alpha, a_array, b_array, c_sycl, c_tbb);\n",
    "  common::print_results(alpha, a_array, b_array, c_sycl, c_tbb);\n",
    "} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! chmod 755 q; chmod 755 ./scripts/run_tasks-solved.sh; if [ -x \"$(command -v qsub)\" ]; then ./q scripts/run_tasks-solved.sh; else ./scripts/run_tasks-solved.sh; fi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using resumable tasks or async_node to share the workload across the CPU and GPU\n",
    "\n",
    "Let's say we only have to compute a single result array. But, we want to get the most out of both the CPU and the GPU by sharing the workload. The most simple alternative is to statically partition the iteration space in two sub-regions and assign the first partition to the GPU and the second one to the CPU:\n",
    "\n",
    "<img src=\"assets/Triad-Suspend_task.png\" width=\"500\">\n",
    "\n",
    "In the next code we introduce several changes:\n",
    "\n",
    "1. We use the `offload_ratio=0.5` variable to indicate that we want to offload to the GPU (using a SYCL queue) 50% of the iteration space and the other 50% to the CPU (that gets processed by a `tbb::parallel_for`)\n",
    "2. We use a different `alpha` for the GPU (`alpha_sycl = 0.5`) and for the CPU (`alpha_tbb = 1.0`). That way, when printing the C array we can easily identify the sub-array updated on the GPU (the *.5 values) and the sub-array updated on the CPU (all integer values).\n",
    "3. We use USM host-allocated arrays (also accessible from the GPU), instead of using sycl::buffer. That way: i) we provide another example that uses USM; ii) the resulting code is simpler; and iii) it may exhibit performance improvements on integrated GPUs that share the global memory with the CPU.\n",
    "4. We use two C arrays (`c_sycl` and `c_tbb`) as in the previous examples, but after the GPU and the CPU are done with their respective duties, we combine the GPU part into the CPU array `c_tbb`). In some cases (USM with fine-grained sharing capabilities) a single C array would do, but for portability sake, we decided to use the safest approach that avoids having the CPU and the GPU concurrently writing in the same array (even if it is in different non-overlapping regions).\n",
    "\n",
    "This is a simple fine-grained CPU+GPU demonstration that may not perform better than a CPU-only or GPU-only alternative, but for other coarser-grained problems this static partitioning of the iteration space can improve performance and/or reduce energy consumption.\n",
    "\n",
    "## Resumable tasks\n",
    "\n",
    "In the next code, we use `tbb::task::suspend()` instead of `tbb::task_group::run()` to avoid blocking a TBB working thread while waiting for the GPU task. Here you can find detailed information about [tbb::suspend_task](https://www.threadingbuildingblocks.org/docs/help/reference/appendices/preview_features/resumable_tasks.html), but you can also refer to slide 27 of the previous presentation.\n",
    "\n",
    "In the current state, a user-defined `AsyncActivity`is created in the `main()` function. At construction time, AsyncActity starts a thread that waits until `submit_flag==true`, then offloads the computation to the GPU, and when the GPU has finished, it sets `submit_flag=false`. `AsyncActivity::submit()` is called in the `main()` function after starting the CPU computation. This member function is the one setting `submit_flat=true` and then spin-waits until the thread completes the GPU work and sets `submit_flag=false`. There is useless thread spinning here, so let's fix it and simplify it using `tbb::task::suspend()`.\n",
    "\n",
    "1. Inspect the code cell below and make the following modifications.\n",
    "  1. STEP A: Inside `main()`, put the call to submit inside of a call to `tbb::task::suspend()`, as in Slide 27 from the previous presentation\n",
    "  2. STEP B: Inside the thread body, remove the `submit_flag=false` and instead use `tbb::task::resume()`.\n",
    "  3. STEP C: Inside `AsyncActivity::submit()` remove the idle spin loop waiting for the GPU to finish (now `tbb::task::suspend()` takes care of waiting)\n",
    "2. Run ▶ the cell in the __Build and Run the modified code__ section below the code snippet to compile and execute the code in the saved file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile lab/triad-hetero-suspend-resume.cpp\n",
    "//==============================================================\n",
    "// Copyright (c) 2020 Intel Corporation\n",
    "//\n",
    "// SPDX-License-Identifier: Apache 2.0\n",
    "// =============================================================\n",
    "\n",
    "#include \"../common/common_utils.hpp\"\n",
    "\n",
    "#include <array>\n",
    "#include <atomic>\n",
    "#include <cmath>\n",
    "#include <iostream>\n",
    "#include <thread>\n",
    "#include <algorithm>\n",
    "\n",
    "#include <CL/sycl.hpp>\n",
    "\n",
    "#include <tbb/blocked_range.h>\n",
    "#include <tbb/task.h>\n",
    "#include <tbb/task_group.h>\n",
    "#include <tbb/parallel_for.h>\n",
    "\n",
    "template<size_t array_size>\n",
    "class AsyncActivity {\n",
    "    float alpha;\n",
    "    const float *a_array, *b_array; \n",
    "    float *c_sycl;\n",
    "    sycl::queue& q;\n",
    "    float offload_ratio;\n",
    "    std::atomic<bool> submit_flag;\n",
    "    tbb::task::suspend_point suspend_point;\n",
    "    std::thread service_thread;\n",
    "\n",
    "public:\n",
    "    AsyncActivity(float alpha_sycl, const float *a, const float *b, float *c, sycl::queue &queue) : \n",
    "      alpha{alpha_sycl}, a_array{a}, b_array{b}, c_sycl{c}, q{queue}, \n",
    "      offload_ratio{0}, submit_flag{false},\n",
    "      service_thread([this] {\n",
    "        // We are in the constructor so this thread is dispatched at AsyncActivity construction\n",
    "        // Wait until the job is submitted into the tbb::suspend_task()\n",
    "        while(!submit_flag) std::this_thread::yield();\n",
    "        // Here submit_flag==true --> DISPATCH GPU computation\n",
    "        std::size_t array_size_sycl = std::ceil(array_size * offload_ratio);\n",
    "        float l_alpha=alpha;\n",
    "        const float *la=a_array, *lb=b_array;\n",
    "        float *lc=c_sycl;\n",
    "        q.submit([&](sycl::handler& h) {            \n",
    "            h.parallel_for(sycl::range<1>{array_size_sycl}, [=](sycl::id<1> index) {\n",
    "              lc[index] = la[index] + lb[index] * l_alpha;\n",
    "            });  \n",
    "        }).wait(); //The thread may spin or block here.\n",
    "  \n",
    "        // Pass a signal into the main thread that the GPU work is completed\n",
    "        // STEP B: remove the submit_flag=false and instead use tbb::task::resume(). \n",
    "        // See https://www.threadingbuildingblocks.org/docs/help/reference/appendices/preview_features/resumable_tasks.html\n",
    "        submit_flag = false;\n",
    "      }) {}\n",
    "\n",
    "    ~AsyncActivity() {\n",
    "        service_thread.join();\n",
    "    }\n",
    "\n",
    "    void submit( float ratio, tbb::task::suspend_point sus_point ) {\n",
    "        offload_ratio = ratio;\n",
    "        suspend_point = sus_point;\n",
    "        submit_flag = true;\n",
    "        // STEP C: remove the idle spin loop on the submit_flag\n",
    "        //         this becomes unecessary once suspend / resume is used\n",
    "        // Now it is necessary to avoid this function returning befor the GPU has finished\n",
    "        while (submit_flag) // Wait until submit_flat==false (The service thread does that after the GPU has finished)\n",
    "          std::this_thread::yield();        \n",
    "    }\n",
    "}; // class AsyncActivity\n",
    "\n",
    "int main() {\n",
    "  \n",
    "  constexpr float ratio = 0.5; // CPU or GPU offload ratio\n",
    "  // We use different alpha coefficients so that \n",
    "  //we can identify the GPU and CPU part if we print c_array result\n",
    "  const float alpha_sycl = 0.5, alpha_tbb = 1.0;  \n",
    "  constexpr size_t array_size = 16;\n",
    "\n",
    "  sycl::queue q{sycl::gpu_selector{}};\n",
    "  std::cout << \"Using device: \" << q.get_device().get_info<sycl::info::device::name>() << '\\n'; \n",
    "\n",
    "  //This host allocation of c comes handy specially for integrated GPUs (CPU and GPU share mem)\n",
    "  float *a_array = malloc_host<float>(array_size, q); \n",
    "  float *b_array = malloc_host<float>(array_size, q); \n",
    "  float *c_sycl = malloc_host<float>(array_size, q);\n",
    "  float *c_tbb = new float[array_size];\n",
    "\n",
    "  // sets array values to 0..N\n",
    "  std::iota(a_array, a_array+array_size,0); \n",
    "  std::iota(b_array, b_array+array_size,0);\n",
    "  \n",
    "  tbb::task_group tg;\n",
    "  AsyncActivity<array_size> activity{alpha_sycl, a_array, b_array, c_sycl, q};\n",
    "\n",
    "  //Spawn a task that runs a parallel_for on the CPU\n",
    "  tg.run([&, alpha_tbb]{\n",
    "   std::size_t i_start = static_cast<std::size_t>(std::ceil(array_size * ratio));\n",
    "   std::size_t i_end = array_size;\n",
    "   tbb::parallel_for(i_start, i_end, [=]( std::size_t index ) {\n",
    "     c_tbb[index] = a_array[index] + alpha_tbb * b_array[index];\n",
    "   });\n",
    "  });\n",
    "\n",
    "  //Spawn another task that asyncrhonously offloads computation to the GPU  \n",
    "  // STEP A: Put the call to submit inside of a call to tbb::task::suspend, as in Slide 27 from the previous presentation\n",
    "  activity.submit(ratio, tbb::task::suspend_point{});\n",
    "\n",
    "  tg.wait();\n",
    "\n",
    "  //Merge GPU result into CPU array\n",
    "  std::size_t gpu_end = static_cast<std::size_t>(std::ceil(array_size * ratio));\n",
    "  std::copy(c_sycl, c_sycl+gpu_end, c_tbb);\n",
    "\n",
    "  common::validate_usm_results(ratio, alpha_sycl, alpha_tbb, a_array, b_array, c_tbb, array_size);\n",
    "  if(array_size<64)\n",
    "    common::print_usm_results(ratio, alpha_sycl, alpha_tbb, a_array, b_array, c_tbb, array_size);\n",
    "\n",
    "  free(a_array,q);\n",
    "  free(b_array,q);\n",
    "  free(c_sycl,q);\n",
    "  delete[] c_tbb;\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build and Run the modified code\n",
    "Select the cell below and click Run ▶ to compile and execute the code above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! chmod 755 q; chmod 755 ./scripts/run_suspend-resume.sh; if [ -x \"$(command -v qsub)\" ]; then ./q scripts/run_suspend-resume.sh; else ./scripts/run_suspend-resume.sh; fi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution (Don't peak unless you have to)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile solutions/triad-hetero-suspend-resume-solved.cpp\n",
    "//==============================================================\n",
    "// Copyright (c) 2020 Intel Corporation\n",
    "//\n",
    "// SPDX-License-Identifier: Apache 2.0\n",
    "// =============================================================\n",
    "\n",
    "#include \"../common/common_utils.hpp\"\n",
    "\n",
    "#include <array>\n",
    "#include <atomic>\n",
    "#include <cmath>\n",
    "#include <iostream>\n",
    "#include <thread>\n",
    "#include <algorithm>\n",
    "\n",
    "#include <CL/sycl.hpp>\n",
    "\n",
    "#include <tbb/blocked_range.h>\n",
    "#include <tbb/task.h>\n",
    "#include <tbb/task_group.h>\n",
    "#include <tbb/parallel_for.h>\n",
    "\n",
    "template<size_t array_size>\n",
    "class AsyncActivity {\n",
    "    float alpha;\n",
    "    const float *a_array, *b_array; \n",
    "    float *c_sycl;\n",
    "    sycl::queue& q;\n",
    "    float offload_ratio;\n",
    "    std::atomic<bool> submit_flag;\n",
    "    tbb::task::suspend_point suspend_point;\n",
    "    std::thread service_thread;\n",
    "\n",
    "public:\n",
    "    AsyncActivity(float alpha_sycl, const float *a, const float *b, float *c, sycl::queue &queue) : \n",
    "      alpha{alpha_sycl}, a_array{a}, b_array{b}, c_sycl{c}, q{queue}, \n",
    "      offload_ratio{0}, submit_flag{false},\n",
    "      service_thread([this] {\n",
    "        // Wait until the job will be submitted into the async activity\n",
    "        while(!submit_flag) std::this_thread::yield();\n",
    "        // Here submit_flag==true --> DISPATCH GPU computation\n",
    "        std::size_t array_size_sycl = std::ceil(array_size * offload_ratio);\n",
    "        float l_alpha=alpha;\n",
    "        const float *la=a_array, *lb=b_array;\n",
    "        float *lc=c_sycl;\n",
    "        q.submit([&](sycl::handler& h) {            \n",
    "            h.parallel_for(sycl::range<1>{array_size_sycl}, [=](sycl::id<1> index) {\n",
    "              lc[index] = la[index] + lb[index] * l_alpha;\n",
    "            });  \n",
    "        }).wait(); //The thread may spin or block here.\n",
    "  \n",
    "        // Pass a signal into the main thread that the GPU work is completed\n",
    "        tbb::task::resume(suspend_point);\n",
    "      }) {}\n",
    "\n",
    "    ~AsyncActivity() {\n",
    "        service_thread.join();\n",
    "    }\n",
    "\n",
    "    void submit( float ratio, tbb::task::suspend_point sus_point ) {\n",
    "        offload_ratio = ratio;\n",
    "        suspend_point = sus_point;\n",
    "        submit_flag = true;\n",
    "    }\n",
    "}; // class AsyncActivity\n",
    "\n",
    "int main() {\n",
    "  \n",
    "  constexpr float ratio = 0.5; // CPU or GPU offload ratio\n",
    "  // We use different alpha coefficients so that \n",
    "  //we can identify the GPU and CPU part if we print c_array result\n",
    "  const float alpha_sycl = 0.5, alpha_tbb = 1.0;  \n",
    "  constexpr size_t array_size = 16;\n",
    "\n",
    "  sycl::queue q{sycl::gpu_selector{}};\n",
    "  std::cout << \"Using device: \" << q.get_device().get_info<sycl::info::device::name>() << '\\n'; \n",
    "\n",
    "  //This host allocation of c comes handy specially for integrated GPUs (CPU and GPU share mem)\n",
    "  float *a_array = malloc_host<float>(array_size, q); \n",
    "  float *b_array = malloc_host<float>(array_size, q); \n",
    "  float *c_sycl = malloc_host<float>(array_size, q);\n",
    "  float *c_tbb = new float[array_size];\n",
    "\n",
    "  // sets array values to 0..N\n",
    "  std::iota(a_array, a_array+array_size,0); \n",
    "  std::iota(b_array, b_array+array_size,0);\n",
    "  \n",
    "  tbb::task_group tg;\n",
    "  AsyncActivity<array_size> activity{alpha_sycl, a_array, b_array, c_sycl, q};\n",
    "\n",
    "  //Spawn a task that runs a parallel_for on the CPU\n",
    "  tg.run([&, alpha_tbb]{\n",
    "   std::size_t i_start = static_cast<std::size_t>(std::ceil(array_size * ratio));\n",
    "   std::size_t i_end = array_size;\n",
    "   tbb::parallel_for(i_start, i_end, [=]( std::size_t index ) {\n",
    "     c_tbb[index] = a_array[index] + alpha_tbb * b_array[index];\n",
    "   });\n",
    "  });\n",
    "\n",
    "  //Spawn another task that asyncrhonously offloads computation to the GPU  \n",
    "    tbb::task::suspend([&]( tbb::task::suspend_point suspend_point ) {\n",
    "     activity.submit(ratio, suspend_point);\n",
    "    });\n",
    "\n",
    "  tg.wait();\n",
    "\n",
    "  //Merge GPU result into CPU array\n",
    "  std::size_t gpu_end = static_cast<std::size_t>(std::ceil(array_size * ratio));\n",
    "  std::copy(c_sycl, c_sycl+gpu_end, c_tbb);\n",
    "\n",
    "  common::validate_usm_results(ratio, alpha_sycl, alpha_tbb, a_array, b_array, c_tbb, array_size);\n",
    "  if(array_size<64)\n",
    "    common::print_usm_results(ratio, alpha_sycl, alpha_tbb, a_array, b_array, c_tbb, array_size);\n",
    "\n",
    "  free(a_array,q);\n",
    "  free(b_array,q);\n",
    "  free(c_sycl,q);\n",
    "  delete[] c_tbb;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! chmod 755 q; chmod 755 ./scripts/run_suspend-resume-solved.sh; if [ -x \"$(command -v qsub)\" ]; then ./q scripts/run_suspend-resume-solved.sh; else ./scripts/run_suspend-resume-solved.sh; fi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using flow::async_node\n",
    "\n",
    "Now let's assume we have a stream of data that is going to be processed in a TBB Flow Graph, and so use a `tbb::task::async_node` instead. As you can see in the figure, our graph has several nodes:\n",
    "\n",
    "<img src=\"assets/Triad-Async_task.png\" width=\"500\">\n",
    "\n",
    "1. The `tbb::flow::input_node` (**in_node**) initializes a struct with the arrays and companion information, initializes A and B, and passes a pointer to that structure to two nodes that will process the arrays in parallel.\n",
    "2. The `tbb::flow::function_node` (**cpu_node**) computes a sub-region of the arrays on the CPU, using a nested `tbb::parallel_for` to distribute the CPU load among the available CPU cores.\n",
    "3. The `tbb::flow::async_node` (**a_node**) dispatches to an AsyncActivity, quite similar to the previous example. As a reference, you can look at the [reference of tbb::flow::async_node](https://www.threadingbuildingblocks.org/docs/help/index.htm#reference/appendices/preview_features/resumable_tasks.html) or at an easier [example](https://link.springer.com/chapter/10.1007/978-1-4842-4398-5_18).\n",
    "4. The `tbb::flow::join_node` (**node_join**) waits until the CPU and the GPU are done.\n",
    "5. The `tbb::flow::function_node` (**out_node**) receives the pointer to the message that contains the resulting array that is checked and printed.\n",
    "\n",
    "In the following code, the `AsyncActivity` wastes a TBB working thread by spinnning until the GPU has finished processing its region of the arrays, much like in the previous exercise. We can certainly do it better:\n",
    "\n",
    "1. Inspect the code cell below and make the following modifications.\n",
    "  1. STEP A: Inside `main()`, in the body of the `a_node`, remove the `try_put` that in this code is necessary to keep it working (it sends a message to the `node_join`). This `try_put` should now be moved to `AsyncActivity` thread, as we do in the next STEP.\n",
    "  2. STEP B: Inside the `AsyncActivity` thread body, remove the `submit_flag=false` and instead use `gateway->try_put(msg)`. We also have to call `gateway->release_wait()` so that we inform the graph, `g`, that there is no need to wait any longer for the `AsyncActivity`.\n",
    "  3. STEP C: Inside `AsyncActivity::submit()` add a call to `gateway.reserve_wait()` to notify the graph that you are dispatching to an asynchronous activity and that the graph has to wait for it.\n",
    "  4. STEP D: Inside `AsyncActivity::submit()` remove the idle spin loop waiting for the GPU to finish (now the `reserve_wait/release_wait` pair takes care of the necessary synchronization).\n",
    "2. Run ▶ the cell in the __Build and Run the modified code__ section below the code snippet to compile and execute the code in the saved file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile lab/triad-hetero-async_node.cpp\n",
    "//==============================================================\n",
    "// Copyright (c) 2020 Intel Corporation\n",
    "//\n",
    "// SPDX-License-Identifier: Apache 2.0\n",
    "// =============================================================\n",
    "\n",
    "#include \"../common/common_utils.hpp\"\n",
    "\n",
    "#include <cmath>  //for std::ceil\n",
    "#include <array>\n",
    "#include <atomic>\n",
    "#include <iostream>\n",
    "#include <thread>\n",
    "\n",
    "#include <CL/sycl.hpp>\n",
    "\n",
    "#include <tbb/blocked_range.h>\n",
    "#include <tbb/flow_graph.h>\n",
    "#include <tbb/global_control.h>\n",
    "#include <tbb/parallel_for.h>\n",
    "\n",
    "constexpr size_t array_size = 16;\n",
    "\n",
    "template<size_t ARRAY_SIZE>\n",
    "struct msg_t {\n",
    "  static constexpr size_t array_size = ARRAY_SIZE;\n",
    "  const float offload_ratio = 0.5;\n",
    "  const float alpha_0 = 0.5;\n",
    "  const float alpha_1 = 1.0;\n",
    "  std::array<float, array_size> a_array;  // input\n",
    "  std::array<float, array_size> b_array;  // input\n",
    "  std::array<float, array_size> c_sycl;   // GPU output\n",
    "  std::array<float, array_size> c_tbb;    // CPU output\n",
    "};\n",
    "\n",
    "using msg_ptr = std::shared_ptr<msg_t<array_size>>;\n",
    "\n",
    "using async_node_t = tbb::flow::async_node<msg_ptr, msg_ptr>;\n",
    "using gateway_t = async_node_t::gateway_type;\n",
    "\n",
    "class AsyncActivity {\n",
    "  msg_ptr msg;\n",
    "  gateway_t* gateway_ptr;\n",
    "  std::atomic<bool> submit_flag;\n",
    "  std::thread service_thread;\n",
    "\n",
    " public:\n",
    "  AsyncActivity() : msg{nullptr}, gateway_ptr{nullptr}, submit_flag{false},\n",
    "    service_thread( [this] {\n",
    "      //Wait until other thread sets submit_flag=true\n",
    "      while( !submit_flag ) std::this_thread::yield();\n",
    "      // Here we go! Dispatch code to the GPU\n",
    "      // Execute the kernel over a portion of the array range\n",
    "      size_t array_size_sycl = std::ceil(msg->a_array.size() * msg->offload_ratio);\n",
    "      {  \n",
    "        sycl::buffer a_buffer{msg->a_array}, b_buffer{msg->b_array}, c_buffer{msg->c_sycl};\n",
    "        sycl::queue q{sycl::gpu_selector{}};\n",
    "        float alpha = msg->alpha_0;\n",
    "        q.submit([&, alpha](sycl::handler& h) {            \n",
    "          sycl::accessor a_accessor{a_buffer, h, sycl::read_only};\n",
    "          sycl::accessor b_accessor{b_buffer, h, sycl::read_only};\n",
    "          sycl::accessor c_accessor{c_buffer, h, sycl::write_only};\n",
    "          h.parallel_for(sycl::range<1>{array_size_sycl}, [=](sycl::id<1> index) {\n",
    "            c_accessor[index] = a_accessor[index] + b_accessor[index] * alpha;\n",
    "          });  \n",
    "        }).wait();\n",
    "      }\n",
    "      // STEP B: Remove the set of submit_flag and replace with\n",
    "      //         a call to try_put on the gateway\n",
    "      //         and a call to release_wait on the gateway\n",
    "      submit_flag = false;\n",
    "    } ) {}\n",
    "\n",
    "  ~AsyncActivity() {\n",
    "    service_thread.join();\n",
    "  }\n",
    "\n",
    "  void submit(msg_ptr m, gateway_t& gateway) {\n",
    "    // STEP C: add a call to gateway.reserve_wait()\n",
    "    msg = m;\n",
    "    gateway_ptr = &gateway;\n",
    "    submit_flag = true;\n",
    "    // STEP D: remove the idle spin loop on the submit_flag\n",
    "    //         this becomes unecessary once reserve_wait / release_wait is used\n",
    "    while (submit_flag)\n",
    "      std::this_thread::yield();\n",
    "  }\n",
    "};\n",
    "\n",
    "int main() {\n",
    "  tbb::flow::graph g;\n",
    "\n",
    "  // Input node:\n",
    "  tbb::flow::input_node<msg_ptr> in_node{g, \n",
    "    [&](tbb::flow_control& fc) -> msg_ptr {\n",
    "      static bool has_run = false;\n",
    "      if (has_run) fc.stop();\n",
    "      has_run = true; // This example only creates a message to feed the Flow Graph\n",
    "      msg_ptr msg = std::make_shared<msg_t<array_size>>();\n",
    "      common::init_input_arrays(msg->a_array, msg->b_array);\n",
    "      return msg;\n",
    "    }\n",
    "  };\n",
    "\n",
    "  // CPU node\n",
    "  tbb::flow::function_node<msg_ptr, msg_ptr> cpu_node{\n",
    "      g, tbb::flow::unlimited, [&](msg_ptr msg) -> msg_ptr {\n",
    "        size_t i_start = static_cast<size_t>(std::ceil(msg->array_size * msg->offload_ratio));\n",
    "        size_t i_end = static_cast<size_t>(msg->array_size);\n",
    "        auto &a_array = msg->a_array, &b_array = msg->b_array, &c_tbb = msg->c_tbb;\n",
    "        float alpha = msg->alpha_1;\n",
    "        tbb::parallel_for(tbb::blocked_range<size_t>{i_start, i_end},\n",
    "          [&, alpha](const tbb::blocked_range<size_t>& r) {\n",
    "            for (size_t i = r.begin(); i < r.end(); ++i)\n",
    "              c_tbb[i] = a_array[i] + alpha * b_array[i];\n",
    "            }\n",
    "        );\n",
    "        return msg;\n",
    "      }};\n",
    "\n",
    "  // async node -- GPU\n",
    "  AsyncActivity async_act;\n",
    "  async_node_t a_node{g, tbb::flow::unlimited,\n",
    "    [&async_act](msg_ptr msg, gateway_t& gateway) {\n",
    "      async_act.submit(msg, gateway);\n",
    "      // STEP A: remove the try_put below since submit will not block\n",
    "      //         In STEP B you will modify AsyncActivity so that it makes the call to try_put instead\n",
    "      gateway.try_put(msg);\n",
    "    }\n",
    "  };\n",
    "\n",
    "  // join node\n",
    "  using join_t = tbb::flow::join_node<std::tuple<msg_ptr, msg_ptr>>;\n",
    "  join_t node_join{g};\n",
    "\n",
    "  // out node\n",
    "  tbb::flow::function_node<join_t::output_type> out_node{g, tbb::flow::unlimited, \n",
    "    [&](const join_t::output_type& two_msgs) {\n",
    "      msg_ptr msg = std::get<0>(two_msgs); //Both msg's point to the same data\n",
    "      //Merge GPU result into CPU array\n",
    "      std::size_t gpu_end = static_cast<std::size_t>(std::ceil(msg->array_size * msg->offload_ratio));\n",
    "      std::copy(msg->c_sycl.begin(), msg->c_sycl.begin()+gpu_end, msg->c_tbb.begin());\n",
    "      common::validate_hetero_results(msg->offload_ratio, msg->alpha_0, msg->alpha_1, \n",
    "                                      msg->a_array, msg->b_array, msg->c_tbb);\n",
    "      if(msg->array_size<=64)\n",
    "        common::print_hetero_results(msg->offload_ratio, msg->alpha_0, msg->alpha_1, \n",
    "                                     msg->a_array, msg->b_array, msg->c_tbb);\n",
    "    }\n",
    "  };  // end of out node\n",
    "\n",
    "  // construct graph\n",
    "  tbb::flow::make_edge(in_node, a_node);\n",
    "  tbb::flow::make_edge(in_node, cpu_node);\n",
    "  tbb::flow::make_edge(a_node, tbb::flow::input_port<0>(node_join));\n",
    "  tbb::flow::make_edge(cpu_node, tbb::flow::input_port<1>(node_join));\n",
    "  tbb::flow::make_edge(node_join, out_node);\n",
    "\n",
    "  in_node.activate();\n",
    "  g.wait_for_all();\n",
    "\n",
    "  return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build and Run the modified code\n",
    "\n",
    "Select the cell below and click Run ▶ to compile and execute the code that you modified above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! chmod 755 q; chmod 755 ./scripts/run_async_node.sh; if [ -x \"$(command -v qsub)\" ]; then ./q scripts/run_async_node.sh; else ./scripts/run_async_node.sh; fi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution (Don't peak unless you have to)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile solutions/triad-hetero-async_node-solved.cpp\n",
    "//==============================================================\n",
    "// Copyright (c) 2020 Intel Corporation\n",
    "//\n",
    "// SPDX-License-Identifier: Apache 2.0\n",
    "// =============================================================\n",
    "\n",
    "#include \"../common/common_utils.hpp\"\n",
    "\n",
    "#include <cmath>  //for std::ceil\n",
    "#include <array>\n",
    "#include <atomic>\n",
    "#include <iostream>\n",
    "#include <thread>\n",
    "\n",
    "#include <CL/sycl.hpp>\n",
    "\n",
    "#include <tbb/blocked_range.h>\n",
    "#include <tbb/flow_graph.h>\n",
    "#include <tbb/global_control.h>\n",
    "#include <tbb/parallel_for.h>\n",
    "\n",
    "constexpr size_t array_size = 16;\n",
    "\n",
    "template<size_t ARRAY_SIZE>\n",
    "struct msg_t {\n",
    "  static constexpr size_t array_size = ARRAY_SIZE;\n",
    "  const float offload_ratio = 0.5;\n",
    "  const float alpha_0 = 0.5;\n",
    "  const float alpha_1 = 1.0;\n",
    "  std::array<float, array_size> a_array;  // input\n",
    "  std::array<float, array_size> b_array;  // input\n",
    "  std::array<float, array_size> c_sycl;   // GPU output\n",
    "  std::array<float, array_size> c_tbb;    // CPU output\n",
    "};\n",
    "\n",
    "using msg_ptr = std::shared_ptr<msg_t<array_size>>;\n",
    "\n",
    "using async_node_t = tbb::flow::async_node<msg_ptr, msg_ptr>;\n",
    "using gateway_t = async_node_t::gateway_type;\n",
    "\n",
    "class AsyncActivity {\n",
    "  msg_ptr msg;\n",
    "  gateway_t* gateway_ptr;\n",
    "  std::atomic<bool> submit_flag;\n",
    "  std::thread service_thread;\n",
    "\n",
    " public:\n",
    "  AsyncActivity() : msg{nullptr}, gateway_ptr{nullptr}, submit_flag{false},\n",
    "    service_thread( [this] {\n",
    "      //Wait until other thread sets submit_flag=true\n",
    "      while( !submit_flag ) std::this_thread::yield();\n",
    "      // Here we go! Dispatch code to the GPU\n",
    "      // Execute the kernel over a portion of the array range\n",
    "      size_t array_size_sycl = std::ceil(msg->a_array.size() * msg->offload_ratio);\n",
    "      {  \n",
    "        sycl::buffer a_buffer{msg->a_array}, b_buffer{msg->b_array}, c_buffer{msg->c_sycl};\n",
    "        sycl::queue q{sycl::gpu_selector{}};\n",
    "        float alpha = msg->alpha_0;\n",
    "        q.submit([&, alpha](sycl::handler& h) {            \n",
    "          sycl::accessor a_accessor{a_buffer, h, sycl::read_only};\n",
    "          sycl::accessor b_accessor{b_buffer, h, sycl::read_only};\n",
    "          sycl::accessor c_accessor{c_buffer, h, sycl::write_only};\n",
    "          h.parallel_for(sycl::range<1>{array_size_sycl}, [=](sycl::id<1> index) {\n",
    "            c_accessor[index] = a_accessor[index] + b_accessor[index] * alpha;\n",
    "          });  \n",
    "        }).wait();\n",
    "      }\n",
    "      gateway_ptr->try_put(msg);\n",
    "      gateway_ptr->release_wait();\n",
    "    } ) {}\n",
    "\n",
    "  ~AsyncActivity() {\n",
    "    service_thread.join();\n",
    "  }\n",
    "\n",
    "  void submit(msg_ptr m, gateway_t& gateway) {\n",
    "    gateway.reserve_wait();\n",
    "    msg = m;\n",
    "    gateway_ptr = &gateway;\n",
    "    submit_flag = true;\n",
    "  }\n",
    "};\n",
    "\n",
    "int main() {\n",
    "  tbb::flow::graph g;\n",
    "\n",
    "  // Input node:\n",
    "  tbb::flow::input_node<msg_ptr> in_node{g, \n",
    "    [&](tbb::flow_control& fc) -> msg_ptr {\n",
    "      static bool has_run = false;\n",
    "      if (has_run) fc.stop();\n",
    "      has_run = true; // This example only creates a message to feed the Flow Graph\n",
    "      msg_ptr msg = std::make_shared<msg_t<array_size>>();\n",
    "      common::init_input_arrays(msg->a_array, msg->b_array);\n",
    "      return msg;\n",
    "    }\n",
    "  };\n",
    "\n",
    "  // CPU node\n",
    "  tbb::flow::function_node<msg_ptr, msg_ptr> cpu_node{\n",
    "      g, tbb::flow::unlimited, [&](msg_ptr msg) -> msg_ptr {\n",
    "        size_t i_start = static_cast<size_t>(std::ceil(msg->array_size * msg->offload_ratio));\n",
    "        size_t i_end = static_cast<size_t>(msg->array_size);\n",
    "        auto &a_array = msg->a_array, &b_array = msg->b_array, &c_tbb = msg->c_tbb;\n",
    "        float alpha = msg->alpha_1;\n",
    "        tbb::parallel_for(tbb::blocked_range<size_t>{i_start, i_end},\n",
    "          [&, alpha](const tbb::blocked_range<size_t>& r) {\n",
    "            for (size_t i = r.begin(); i < r.end(); ++i)\n",
    "              c_tbb[i] = a_array[i] + alpha * b_array[i];\n",
    "            }\n",
    "        );\n",
    "        return msg;\n",
    "      }};\n",
    "\n",
    "  // async node -- GPU\n",
    "  AsyncActivity async_act;\n",
    "  async_node_t a_node{g, tbb::flow::unlimited,\n",
    "    [&async_act](msg_ptr msg, gateway_t& gateway) {\n",
    "      async_act.submit(msg, gateway);\n",
    "    }\n",
    "  };\n",
    "\n",
    "  // join node\n",
    "  using join_t = tbb::flow::join_node<std::tuple<msg_ptr, msg_ptr>>;\n",
    "  join_t node_join{g};\n",
    "\n",
    "  // out node\n",
    "  tbb::flow::function_node<join_t::output_type> out_node{g, tbb::flow::unlimited, \n",
    "    [&](const join_t::output_type& two_msgs) {\n",
    "      msg_ptr msg = std::get<0>(two_msgs); //Both msg's point to the same data\n",
    "      //Merge GPU result into CPU array\n",
    "      std::size_t gpu_end = static_cast<std::size_t>(std::ceil(msg->array_size * msg->offload_ratio));\n",
    "      std::copy(msg->c_sycl.begin(), msg->c_sycl.begin()+gpu_end, msg->c_tbb.begin());\n",
    "      common::validate_hetero_results(msg->offload_ratio, msg->alpha_0, msg->alpha_1, \n",
    "                                      msg->a_array, msg->b_array, msg->c_tbb);\n",
    "      if(msg->array_size<=64)\n",
    "        common::print_hetero_results(msg->offload_ratio, msg->alpha_0, msg->alpha_1, \n",
    "                                     msg->a_array, msg->b_array, msg->c_tbb);\n",
    "    }\n",
    "  };  // end of out node\n",
    "\n",
    "  // construct graph\n",
    "  tbb::flow::make_edge(in_node, a_node);\n",
    "  tbb::flow::make_edge(in_node, cpu_node);\n",
    "  tbb::flow::make_edge(a_node, tbb::flow::input_port<0>(node_join));\n",
    "  tbb::flow::make_edge(cpu_node, tbb::flow::input_port<1>(node_join));\n",
    "  tbb::flow::make_edge(node_join, out_node);\n",
    "\n",
    "  in_node.activate();\n",
    "  g.wait_for_all();\n",
    "\n",
    "  return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! chmod 755 q; chmod 755 ./scripts/run_async_node-solved.sh; if [ -x \"$(command -v qsub)\" ]; then ./q scripts/run_async_node-solved.sh; else ./scripts/run_async_node-solved.sh; fi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Congrats on getting here!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7 (Intel® oneAPI)",
   "language": "python",
   "name": "c009-intel_distribution_of_python_3_oneapi-beta05-python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
