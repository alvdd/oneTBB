{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to oneDPL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Sections\n",
    "- _Code_: [Hello, PSTL!](#Hello,-PSTL!)\n",
    "- _Code_: [Fancy Iterators](#Fancy-iterators)\n",
    "- _Code_: [Monte Carlo Example](#Monte-Carlo-Pi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# Hello, PSTL!\n",
    "\n",
    "To begin, we'll build and run a very simple C++ application that invokes the classic STL algorithm ``sort`` on an input sequence of random integers.\n",
    "\n",
    "To make this exercise more interesting we parallelize our sort at a high-level. To do that, we simply add a parallel execution policy and take the algorithm for a second run. Adding a parallel execution policy is as easy as inserting `std::execution::par` as a first parameter. This tells the compiler and runtime that it's safe to execute iterations in parallel on multiple threads of execution. It's a hint by the user but not a mandatory requirement to run any optimizations for the runtime. A high-quality implementation like Parallel STL with TBB as a backend could benefit by splitting the sort into finer grained tasks. Those can be executed by TBB's scheduler in a multithreaded fashion. Of course that adds some overhead, such as maintaining a thread pool or synchronize the tasks. So it's not advised to add a parallel execution policy to each and every algorithm. It's a user's (Yes that's you) responsibility to ensure a parallel optimization hint is legal and beneficial.\n",
    "\n",
    "Consequently, in our simple example we time both runs so that we can compare them. Please note that intentionally any warm-up for the parallel execution is skipped.\n",
    "\n",
    "Inspect the code below - there are no modifications necessary. Run the first cell to create the file, then run the cell below it to compile and execute the code.\n",
    "1. Inspect the code cell below, then click run ▶ to save the code to a file\n",
    "2. Run ▶ the cell in the __Build and Run__ section below the code snippet to compile and execute the code in the saved file\n",
    "\n",
    "What are your expectations regarding the speedup? We're likely running on an system with two Intel Xeon processor with 6 cores each (12 hardware threads) in the current DevCloud configuration, as of Sept'20. \n",
    "\n",
    "What happens if we decrease the size of the input sequence to let's say 50? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile lab/sort.cpp\n",
    "//==============================================================\n",
    "// Copyright (c) 2020 Intel Corporation\n",
    "//\n",
    "// SPDX-License-Identifier: Apache-2.0\n",
    "// =============================================================\n",
    "\n",
    "#include <chrono>\n",
    "#include <iostream>\n",
    "#include <vector>\n",
    "#include <random>\n",
    "\n",
    "#include <oneapi/dpl/execution>\n",
    "#include <oneapi/dpl/algorithm>\n",
    "\n",
    "int main() {\n",
    "  std::default_random_engine e1(0);\n",
    "  std::uniform_int_distribution<int> d(0, 5000);\n",
    "\n",
    "  // initialize input vector with random numbers\n",
    "  std::vector<int> data(5000000);\n",
    "  for(auto& i:data) {\n",
    "    i = d(e1);\n",
    "  }\n",
    "\n",
    "  {\n",
    "    std::vector<int> input(data);\n",
    "    std::cout << \"Running sequentially:\\n\";\n",
    "    auto st0 = std::chrono::high_resolution_clock::now();\n",
    "    std::sort(input.begin(), input.end());\n",
    "    auto st1 = std::chrono::high_resolution_clock::now();\n",
    "    std::cout << \"Serial time   = \" << 1e-9 * (st1-st0).count() << \" seconds\\n\";\n",
    "  }\n",
    "\n",
    "  {\n",
    "    std::vector<int> input(data);\n",
    "    std::cout << \"\\nRunning in parallel:\\n\";\n",
    "    auto pt0 = std::chrono::high_resolution_clock::now();\n",
    "    std::sort(std::execution::par, input.begin(), input.end());\n",
    "    auto pt1 = std::chrono::high_resolution_clock::now();\n",
    "    std::cout << \"Parallel time = \" << 1e-9 * (pt1-pt0).count() << \" seconds\\n\";\n",
    "  }\n",
    "    \n",
    "  return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile scripts/run_sort.sh\n",
    "#!/bin/bash\n",
    "#==========================================\n",
    "# Copyright (c) 2020 Intel Corporation\n",
    "#\n",
    "# SPDX-License-Identifier: Apache-2.0\n",
    "#==========================================\n",
    "\n",
    "source /opt/intel/inteloneapi/setvars.sh > /dev/null 2>&1\n",
    "/bin/echo \"##\" $(whoami) is compiling oneDPL example\n",
    "rm -rf bin/sort\n",
    "dpcpp lab/sort.cpp -o bin/sort -tbb\n",
    "bin/sort"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select the cell below and click Run ▶ to compile and execute the code above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! chmod 755 q; chmod 755 scripts/run_sort.sh; if [ -x \"$(command -v qsub)\" ]; then ./q scripts/run_sort.sh; else ./scripts/run_sort.sh; fi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# Fancy iterators\n",
    "\n",
    "Next we want to demonstrate how the applicability of classic STL algorithms can be extended by special iterators, so called fancy iterators. Imagine the following problem: A logic sequence of pairs (key and data elements) has to be sorted according to its key element. The elements are stored in two containers, one vector that holds the keys and another vector that holds the data elements.\n",
    "\n",
    "oneDPL as part of Intel oneAPI supports fancy iterators that can be used with the C++17 parallel algorithms. For example a `counting_iterator` is provided that represents a linear increasing sequence. With one big advantage compared to a classic vector that holds the data. This can actually save memory bandwidth. Because the sequence does not need a representation in memory.\n",
    "\n",
    "In our example we use a `zip_iterator` that can be used to tie two or even multiple sequences. The resulting iterator can now be used as an input or output of STL algorithm. Like in the following example.\n",
    "\n",
    "Complete the instructions in the following code template to implement a sort by key using a zip iterator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile lab/fancy_sort.cpp\n",
    "//==============================================================\n",
    "// Copyright (c) 2020 Intel Corporation\n",
    "//\n",
    "// SPDX-License-Identifier: Apache-2.0\n",
    "// =============================================================\n",
    "\n",
    "#include <chrono>\n",
    "#include <iostream>\n",
    "#include <vector>\n",
    "\n",
    "#include <oneapi/dpl/iterator>\n",
    "#include <oneapi/dpl/execution>\n",
    "#include <oneapi/dpl/algorithm>\n",
    "\n",
    "int main() {\n",
    "\n",
    "  {\n",
    "    using oneapi::dpl::make_zip_iterator;\n",
    "    std::vector<int> keys = { 0, 1, 0, 1, 0, 1, 0, 1, 0, 1};\n",
    "    std::vector<int> data = { 0, 1, 2, 3, 4, 5, 6, 7, 8, 9};\n",
    "    // *** Step 1: create a zip iterator with iterator to begin of keys and data as input.\n",
    "    auto zip_in = data.begin();\n",
    "    const size_t n = std::distance(keys.begin(), keys.end());  \n",
    "\n",
    "    std::cout << \"\\nRunning in parallel:\\n\";\n",
    "    auto pt0 = std::chrono::high_resolution_clock::now();\n",
    "    // *** Step 2: Replace left and right side of comparison with get<>() to extract and compare key\n",
    "    auto custom_func = [](auto l, auto r){ using std::get; return l < r;};\n",
    "    // *** Step 3: Replace all instances of iterator data.begin() with your zip iterator\n",
    "    std::sort(std::execution::par, zip_in, zip_in + n, custom_func);\n",
    "    auto pt1 = std::chrono::high_resolution_clock::now();\n",
    "    std::cout << \"Parallel time = \" << 1e-9 * (pt1-pt0).count() << \" seconds\\n\";\n",
    "      \n",
    "    std::cout << \"Sorted keys:\" ;\n",
    "    for(auto i:keys)\n",
    "      std::cout << i << \" \";\n",
    "    std::cout << \"\\n\" << \"Sorted data:\";\n",
    "    for(auto i:data)\n",
    "      std::cout << i << \" \";\n",
    "    std::cout << \"\\n\";\n",
    "  }\n",
    "    \n",
    "  return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution (Don't peak unless you have to)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%%writefile lab/fancy_sort.cpp\n",
    "//==============================================================\n",
    "// Copyright (c) 2020 Intel Corporation\n",
    "//\n",
    "// SPDX-License-Identifier: Apache-2.0\n",
    "// =============================================================\n",
    "\n",
    "#include <chrono>\n",
    "#include <iostream>\n",
    "#include <vector>\n",
    "\n",
    "#include <oneapi/dpl/iterator>\n",
    "#include <oneapi/dpl/execution>\n",
    "#include <oneapi/dpl/algorithm>\n",
    "\n",
    "int main() {\n",
    "\n",
    "  {\n",
    "    using oneapi::dpl::make_zip_iterator;\n",
    "    std::vector<int> keys = { 0, 1, 0, 1, 0, 1, 0, 1, 0, 1};\n",
    "    std::vector<int> data = { 0, 1, 2, 3, 4, 5, 6, 7, 8, 9};\n",
    "    auto zip_in = make_zip_iterator(keys.begin(), data.begin());\n",
    "    const size_t n = std::distance(keys.begin(), keys.end());  \n",
    "    auto custom_func = [](auto l, auto r){ using std::get; return get<0>(l) < get<0>(r);};\n",
    "      \n",
    "    std::cout << \"\\nRunning in parallel:\\n\";\n",
    "    auto pt0 = std::chrono::high_resolution_clock::now();\n",
    "    std::sort(std::execution::par, zip_in, zip_in + n, custom_func);\n",
    "    auto pt1 = std::chrono::high_resolution_clock::now();\n",
    "    std::cout << \"Parallel time = \" << 1e-9 * (pt1-pt0).count() << \" seconds\\n\";\n",
    "      \n",
    "    std::cout << \"Sorted keys:\" ;\n",
    "    for(auto i:keys)\n",
    "      std::cout << i << \" \";\n",
    "    std::cout << \"\\n\" << \"Sorted data:\";\n",
    "    for(auto i:data)\n",
    "      std::cout << i << \" \";\n",
    "    std::cout << \"\\n\";\n",
    "  }\n",
    "    \n",
    "  return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile scripts/run_fancy_sort.sh\n",
    "#!/bin/bash\n",
    "#==========================================\n",
    "# Copyright (c) 2020 Intel Corporation\n",
    "#\n",
    "# SPDX-License-Identifier: Apache-2.0\n",
    "#==========================================\n",
    "\n",
    "source /opt/intel/inteloneapi/setvars.sh > /dev/null 2>&1\n",
    "/bin/echo \"##\" $(whoami) is compiling oneDPL example\n",
    "rm -rf bin/fancy_sort\n",
    "dpcpp lab/fancy_sort.cpp -o bin/fancy_sort -tbb\n",
    "bin/fancy_sort"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select the cell below and click Run ▶ to compile and execute the code above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! chmod 755 q; chmod 755 scripts/run_fancy_sort.sh; if [ -x \"$(command -v qsub)\" ]; then ./q scripts/run_fancy_sort.sh; else ./scripts/run_fancy_sort.sh; fi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# Monte Carlo Pi\n",
    "\n",
    "Let's recreate the Direct programming DPC++ example using the DPC++ library (oneDPL). We'll use the algorithm ``transform_reduce``, which can for example be used to parallelize the computation of a dot product, by applying a parallel execution policy. Well or the monte carlo simulation computing PI.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile lab/monte_carlo_pi.cpp\n",
    "//==============================================================\n",
    "// Copyright (c) 2020 Intel Corporation\n",
    "//\n",
    "// SPDX-License-Identifier: Apache-2.0\n",
    "// =============================================================\n",
    "\n",
    "#include <chrono>\n",
    "#include <iostream>\n",
    "#include <vector>\n",
    "\n",
    "#include <CL/sycl.hpp>\n",
    "\n",
    "#include <oneapi/dpl/random>\n",
    "\n",
    "#include <oneapi/dpl/iterator>\n",
    "#include <oneapi/dpl/execution>\n",
    "#include <oneapi/dpl/algorithm>\n",
    "\n",
    "#define N 500\n",
    "#define LOCAL_N 1000\n",
    "\n",
    "#define SEED 667\n",
    "#define PI 3.1415926535897932384626433832795\n",
    "\n",
    "int main() {\n",
    "    \n",
    "    using oneapi::dpl::counting_iterator;\n",
    "    double estimated_pi = 3.14;\n",
    "    {\n",
    "        sycl::queue q; //(sycl::gpu_selector{});\n",
    "        auto policy = oneapi::dpl::execution::make_device_policy(q);\n",
    " \n",
    "        auto sum = transform_reduce( policy, counting_iterator<int>(0), counting_iterator<int>(N), 0, std::plus<float>{}\n",
    "                                    , [=](auto n){\n",
    "                                        float local_sum = 0.0f;\n",
    "                                        // Get random coords\n",
    "                                        oneapi::std::minstd_rand engine(SEED, n);\n",
    "                                        oneapi::std::uniform_real_distribution<float> distr(-1.0f,1.0f);\n",
    "                                        for(int i = 0; i < LOCAL_N; ++i) {\n",
    "                                            float x = distr(engine);\n",
    "                                            float y = distr(engine);\n",
    "                                            auto hypotenuse_sqr = x * x + y * y;\n",
    "                                            if (hypotenuse_sqr <= 1.0)\n",
    "                                                local_sum += 1.0;\n",
    "                                        }\n",
    "                                        return local_sum / (float)LOCAL_N;\n",
    "                                    });\n",
    "\n",
    "        estimated_pi = 4.0 * (float)sum / N;\n",
    "    }\n",
    "        \n",
    "    // Printing Results\n",
    "    std::cout << \"Estimated value of Pi = \" << estimated_pi << std::endl;\n",
    "    std::cout << \"Exact value of Pi = \" << PI << std::endl;\n",
    "    std::cout << \"Absolute error = \" << fabs( PI-estimated_pi ) << std::endl;\n",
    "\n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile scripts/run_monte_carlo_pi.sh\n",
    "#!/bin/bash\n",
    "#==========================================\n",
    "# Copyright (c) 2020 Intel Corporation\n",
    "#\n",
    "# SPDX-License-Identifier: Apache-2.0\n",
    "#==========================================\n",
    "\n",
    "source /opt/intel/inteloneapi/setvars.sh > /dev/null 2>&1\n",
    "/bin/echo \"##\" $(whoami) is compiling oneDPL example\n",
    "rm -rf bin/monte_carlo_pi\n",
    "dpcpp lab/monte_carlo_pi.cpp -o bin/monte_carlo_pi -tbb\n",
    "bin/monte_carlo_pi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select the cell below and click Run ▶ to compile and execute the code above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! chmod 755 q; chmod 755 scripts/run_monte_carlo_pi.sh; if [ -x \"$(command -v qsub)\" ]; then ./q scripts/run_monte_carlo_pi.sh; else ./scripts/run_monte_carlo_pi.sh; fi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7 (Intel® oneAPI)",
   "language": "python",
   "name": "c009-intel_distribution_of_python_3_oneapi-beta05-python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
